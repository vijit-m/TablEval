# BreakingBERT@IITK at SemEval-2021
# Introduction

Task 9 : Statement Verification and Evidence Finding with Tables

Recently, there has been an interest in factual
verification and prediction over structured data
like tables and graphs. To circumvent any false
news incident, it is necessary to not only model
and predict over structured data efficiently but
also to explain those predictions. In this paper, as part of the SemEval-2021 Task 9, we
tackle the problem of fact verification and evidence finding over tabular data. There are two
subtasks. Given a table and a statement/fact,
subtask A determines whether the statement is
inferred from the tabular data, and subtask B
determines which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state-of-the-art approaches over the given SemTabFact dataset.
We also propose a novel approach CellBERT
to solve evidence finding as a form of the Natural Language Inference task. We obtain a 3-
way F1 score of 0.69 on subtask A and an F1
score of 0.65 on subtask B.

# Corpus Description

The corpus has two versions of training data:
- [Manual Dataset](https://drive.google.com/file/d/1yObzEEZJ8qM7ZjrMcbtKZ-jofpL820ft/view)
- [Autogenerated Dataset](https://drive.google.com/file/d/1fz-g3wmAIwav_wQoF9t64NXEuQwyInZq/view)

For more information [refer this](https://sites.google.com/view/sem-tab-facts)

# Requirements
- Python 3.5
- Ujson 1.35
- Pytorch 1.2.0
- Pytorch_Pretrained_Bert 0.6.2 (Huggingface Implementation)
- Pandas
- tqdm-4.35
- TensorboardX
- unidecode
- nltk: wordnet, averaged_perceptron_tagger

# Preprocessing Data
